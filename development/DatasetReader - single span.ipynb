{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 4.28 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from libs.dataset_readers.ssqa_span_reader import SSQASpanReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "reader = SSQASpanReader(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7174ec929934523a053a7f7e2d16aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.07 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../data/ssqa_multiple_choice_span/test.json\"\n",
    "dataset = reader.read(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.32 ms\n"
     ]
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Instance with fields:\n",
      " \t question_with_context: TextField of length 72 with text: \n",
      " \t\t[[CLS], 班, 级, 公, 约, 在, 何, 时, 决, 定, ？, [SEP], 班, 级, 自, 治, 就, 是, 透, 过, 班, 会, ，, 订, 定, 班, 级, 公, 约, 、,\n",
      "\t\t选, 举, 班, 级, 干, 部, 、, 讨, 论, 班, 级, 事, 务, 、, 安, 排, 班, 级, 活, 动, ，, 学, 习, 自, 己, 管, 理, 班, 级, 事, 务, ，, 让,\n",
      "\t\t班, 级, 表, 现, 得, 更, 好, 。, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t answer_span: SpanField with spans: (20, 21). \n",
      " \t context_span: SpanField with spans: (12, 70). \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "1 Instance with fields:\n",
      " \t question_with_context: TextField of length 85 with text: \n",
      " \t\t[[CLS], 班, 级, 公, 约, 由, 谁, 来, 订, 定, ？, [SEP], 班, 级, 公, 约, 是, 全, 班, 同, 学, 共, 同, 订, 定, ，, 用, 以, 约, 束,\n",
      "\t\t每, 个, 人, 的, 行, 为, 。, 订, 定, 班, 级, 公, 约, 时, ，, 要, 考, 虑, 全, 班, 同, 学, 的, 需, 要, ，, 制, 定, 明, 确, 、, 公, 平,\n",
      "\t\t且, 具, 体, 可, 行, 的, 规, 则, ，, 让, 全, 班, 同, 学, 都, 能, 确, 实, 遵, 守, 。, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t answer_span: SpanField with spans: (17, 20). \n",
      " \t context_span: SpanField with spans: (12, 83). \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "2 Instance with fields:\n",
      " \t question_with_context: TextField of length 86 with text: \n",
      " \t\t[[CLS], 班, 级, 公, 约, 的, 功, 能, 是, 什, 么, ？, [SEP], 班, 级, 公, 约, 是, 全, 班, 同, 学, 共, 同, 订, 定, ，, 用, 以, 约,\n",
      "\t\t束, 每, 个, 人, 的, 行, 为, 。, 订, 定, 班, 级, 公, 约, 时, ，, 要, 考, 虑, 全, 班, 同, 学, 的, 需, 要, ，, 制, 定, 明, 确, 、, 公,\n",
      "\t\t平, 且, 具, 体, 可, 行, 的, 规, 则, ，, 让, 全, 班, 同, 学, 都, 能, 确, 实, 遵, 守, 。, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t answer_span: SpanField with spans: (29, 36). \n",
      " \t context_span: SpanField with spans: (13, 84). \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "3 Instance with fields:\n",
      " \t question_with_context: TextField of length 88 with text: \n",
      " \t\t[[CLS], 订, 定, 班, 级, 公, 约, 的, 原, 则, 是, 什, 么, ？, [SEP], 班, 级, 公, 约, 是, 全, 班, 同, 学, 共, 同, 订, 定, ，, 用,\n",
      "\t\t以, 约, 束, 每, 个, 人, 的, 行, 为, 。, 订, 定, 班, 级, 公, 约, 时, ，, 要, 考, 虑, 全, 班, 同, 学, 的, 需, 要, ，, 制, 定, 明, 确,\n",
      "\t\t、, 公, 平, 且, 具, 体, 可, 行, 的, 规, 则, ，, 让, 全, 班, 同, 学, 都, 能, 确, 实, 遵, 守, 。, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t answer_span: SpanField with spans: (48, 57). \n",
      " \t context_span: SpanField with spans: (15, 86). \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "4 Instance with fields:\n",
      " \t question_with_context: TextField of length 89 with text: \n",
      " \t\t[[CLS], 班, 级, 公, 约, 的, 内, 容, 应, 以, 什, 么, 为, 主, ？, [SEP], 班, 级, 公, 约, 是, 全, 班, 同, 学, 共, 同, 订, 定, ，,\n",
      "\t\t用, 以, 约, 束, 每, 个, 人, 的, 行, 为, 。, 订, 定, 班, 级, 公, 约, 时, ，, 要, 考, 虑, 全, 班, 同, 学, 的, 需, 要, ，, 制, 定, 明,\n",
      "\t\t确, 、, 公, 平, 且, 具, 体, 可, 行, 的, 规, 则, ，, 让, 全, 班, 同, 学, 都, 能, 确, 实, 遵, 守, 。, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t answer_span: SpanField with spans: (50, 58). \n",
      " \t context_span: SpanField with spans: (16, 87). \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "time: 3.47 ms\n"
     ]
    }
   ],
   "source": [
    "print(0, dataset[0])\n",
    "print(1, dataset[1])\n",
    "print(2, dataset[2])\n",
    "print(3, dataset[3])\n",
    "print(4, dataset[4])\n",
    "# print(5, dataset[5])\n",
    "# print(6, dataset[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0], dataset[0][\"metadata\"].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of libs.dataset_readers.ssqa_singlespan failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/opt/conda/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/workspace/GNN-based SSQA/libs/dataset_readers/ssqa_singlespan.py\", line 26, in <module>\n",
      "    class SSQASinglespanReader(DatasetReader):\n",
      "  File \"/home/user/.local/lib/python3.7/site-packages/allennlp/common/registrable.py\", line 123, in add_subclass_to_registry\n",
      "    raise ConfigurationError(message)\n",
      "allennlp.common.checks.ConfigurationError: Cannot register ssqa as DatasetReader; name already in use for SSQASinglespanReader\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.7, pytest-6.0.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /workspace/GNN-based SSQA\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "tests/dataset_readers/ssqa_singlespan_test.py \u001b[32m.\u001b[0m\u001b[33m                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:106\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    class DatasetV2(collections.Iterable, tracking_base.Trackable,\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/utils/testing.py:21\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/graphql/type/directives.py:62\n",
      "  /opt/conda/lib/python3.7/site-packages/graphql/type/directives.py:62: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    assert isinstance(args, collections.Mapping), '{} args must be a dict with argument names as keys.'.format(name)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/graphql/type/typemap.py:1\n",
      "  /opt/conda/lib/python3.7/site-packages/graphql/type/typemap.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import OrderedDict, Sequence, defaultdict\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "\u001b[33m======================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m5 warnings\u001b[0m\u001b[33m in 7.34s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/dataset_readers/ssqa_singlespan_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test create answer span position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, Optional, Iterable\n",
    "\n",
    "from allennlp.data.tokenizers import Token, PretrainedTransformerTokenizer\n",
    "from allennlp_models.rc.dataset_readers.utils import char_span_to_token_span\n",
    "from allennlp.common.util import sanitize_wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We 0\n",
      "also 3\n",
      "support 8\n",
      "limiting 16\n",
      "the 25\n",
      "maximum 29\n",
      "length 37\n",
      "for 44\n",
      "the 48\n",
      "question 52\n",
      ". 60\n",
      "and 62\n",
      "re 66\n",
      "##ct 68\n",
      "##ang 70\n",
      "##le 73\n"
     ]
    }
   ],
   "source": [
    "context = \"We also support limiting the maximum length for the question. and rectangle\"\n",
    "\n",
    "tokenizer = PretrainedTransformerTokenizer(\"bert-base-cased\", add_special_tokens=False)\n",
    "\n",
    "def tokenize_slice(start: int, end: int) -> Iterable[Token]:\n",
    "    text_to_tokenize = context[start:end]\n",
    "    if start - 1 >= 0 and context[start - 1].isspace():\n",
    "        prefix = \"a \"  # must end in a space, and be short so we can be sure it becomes only one token\n",
    "        wordpieces = tokenizer.tokenize(prefix + text_to_tokenize)\n",
    "        for wordpiece in wordpieces:\n",
    "            if wordpiece.idx is not None:\n",
    "                wordpiece.idx -= len(prefix)\n",
    "        return wordpieces[1:]\n",
    "    else:\n",
    "        return tokenizer.tokenize(text_to_tokenize)\n",
    "\n",
    "tokenized_context = []\n",
    "token_start = 0\n",
    "for i, c in enumerate(context):\n",
    "    if c.isspace():\n",
    "        for wordpiece in tokenize_slice(token_start, i):\n",
    "            if wordpiece.idx is not None:\n",
    "                wordpiece.idx += token_start\n",
    "            tokenized_context.append(wordpiece)\n",
    "        token_start = i + 1\n",
    "for wordpiece in tokenize_slice(token_start, len(context)):\n",
    "    if wordpiece.idx is not None:\n",
    "        wordpiece.idx += token_start\n",
    "    tokenized_context.append(wordpiece)\n",
    "    \n",
    "for tok in tokenized_context:\n",
    "    print(tok, tok.idx)\n",
    "    \n",
    "# We 0\n",
    "# also 3\n",
    "# support 8\n",
    "# limiting 16\n",
    "# the 25\n",
    "# maximum 29\n",
    "# length 37\n",
    "# for 44\n",
    "# the 48\n",
    "# question 52\n",
    "# . 60\n",
    "# and 62\n",
    "# re 66\n",
    "# ##ct 68\n",
    "# ##ang 70\n",
    "# ##le 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (3, 7),\n",
       " (8, 15),\n",
       " (16, 24),\n",
       " (25, 28),\n",
       " (29, 36),\n",
       " (37, 43),\n",
       " (44, 47),\n",
       " (48, 51),\n",
       " (52, 60),\n",
       " (60, 61),\n",
       " (62, 65),\n",
       " (66, 68),\n",
       " (68, 70),\n",
       " (70, 73),\n",
       " (73, 75)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (t.idx, t.idx + len(sanitize_wordpiece(t.text))) if t.idx is not None else None\n",
    "    for t in tokenized_context\n",
    "]\n",
    "\n",
    "# [(0, 2),\n",
    "#  (3, 7),\n",
    "#  (8, 15),\n",
    "#  (16, 24),\n",
    "#  (25, 28),\n",
    "#  (29, 36),\n",
    "#  (37, 43),\n",
    "#  (44, 47),\n",
    "#  (48, 51),\n",
    "#  (52, 60),\n",
    "#  (60, 61),\n",
    "#  (62, 65),\n",
    "#  (66, 68),\n",
    "#  (68, 70),\n",
    "#  (70, 73),\n",
    "#  (73, 75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3), True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_answer_offset = 16\n",
    "answers = \"limiting\"\n",
    "\n",
    "char_span_to_token_span(\n",
    "    [\n",
    "        (t.idx, t.idx + len(sanitize_wordpiece(t.text))) if t.idx is not None else None\n",
    "        for t in tokenized_context\n",
    "    ],\n",
    "    (first_answer_offset, first_answer_offset + len(answers[0])),\n",
    ")\n",
    "\n",
    "# ((3, 3), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.sequence_pair_end_tokens)\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
